{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-nk-MZiKDWE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kWiSNU8bO8Ee"
   },
   "outputs": [],
   "source": [
    "f0 = open(\"1661-0.txt\", \"r\")\n",
    "f1 = open('pg5200.txt', 'r')\n",
    "lines0 = [i for i in f0]\n",
    "lines1 = [i for i in f1]\n",
    "lines = lines0 + lines1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-2DBsXuPP5q",
    "outputId": "691089c3-0629-40f5-f72a-3834d03c0a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
      "\n",
      "The Last Line:  subscribe to our email newsletter to hear about new eBooks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The First Line: \", lines[1])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "wPztLm3Wz6IZ",
    "outputId": "1f25f94f-e2f5-468c-bb0e-6cfe420e9ff0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle  This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"\"\n",
    "\n",
    "for i in lines:\n",
    "  words = ' '.join(c for c in lines if not c.isdigit()) # converting all the lines in the text files to a corpus or text document\n",
    "    \n",
    "words = words.replace('\\n', '').replace('\\ufeff', '').replace(\"\\\\\", '')\n",
    "words[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCtZDjhz0YSJ",
    "outputId": "cd280176-d3a0-42bb-f1c1-df8d6c4e6072"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136561"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer() #convert the words to a a sequences of integers\n",
    "tokenizer.fit_on_texts([words]) #fit on our corpus\n",
    "sequences = tokenizer.texts_to_sequences([words])[0]\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tumIhzOz8qoH",
    "outputId": "602fc456-14f8-4984-999d-18aa96c76fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9648\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8KclYFw8VAk",
    "outputId": "c68df6fd-b32e-44bc-9fac-a9c2154c6773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 5442], [5442, 1], [1, 1215], [1215, 4], [4, 169], [169, 42]]\n"
     ]
    }
   ],
   "source": [
    "#we create training data based on each word in our array of sequences and the next word to follow\n",
    "training_data = []\n",
    "for i in range(1, len(sequences)):\n",
    "    training_data.append(sequences[i-1:i+1])\n",
    "print(training_data[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qNAOcy1ApsA",
    "outputId": "6f641968-2e16-417f-a7f1-a6abab5c86b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If X:101, then y:5442\n"
     ]
    }
   ],
   "source": [
    "X = np.array([i[0] for i in training_data])\n",
    "y = np.array([i[1] for i in training_data])\n",
    "print(f\"If X:{X[0]}, then y:{y[0]}\")\n",
    "y = keras.utils.to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkFUMGWVDhfE",
    "outputId": "739c4cfb-1661-41e0-8a5a-18c8aa3ca3c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "325/325 [==============================] - 152s 461ms/step - loss: 6.7798 - accuracy: 0.0515 - val_loss: 6.8655 - val_accuracy: 0.0513\n",
      "Epoch 2/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 6.4210 - accuracy: 0.0516 - val_loss: 6.8643 - val_accuracy: 0.0513\n",
      "Epoch 3/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 6.2628 - accuracy: 0.0561 - val_loss: 6.4813 - val_accuracy: 0.0767\n",
      "Epoch 4/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 5.9526 - accuracy: 0.0807 - val_loss: 6.1818 - val_accuracy: 0.0986\n",
      "Epoch 5/50\n",
      "325/325 [==============================] - 149s 459ms/step - loss: 5.6526 - accuracy: 0.1070 - val_loss: 5.9670 - val_accuracy: 0.1258\n",
      "Epoch 6/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 5.4360 - accuracy: 0.1215 - val_loss: 5.8856 - val_accuracy: 0.1353\n",
      "Epoch 7/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 5.2875 - accuracy: 0.1299 - val_loss: 5.8208 - val_accuracy: 0.1434\n",
      "Epoch 8/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 5.1811 - accuracy: 0.1370 - val_loss: 5.8041 - val_accuracy: 0.1508\n",
      "Epoch 9/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 5.0967 - accuracy: 0.1431 - val_loss: 5.7877 - val_accuracy: 0.1539\n",
      "Epoch 10/50\n",
      "325/325 [==============================] - 148s 456ms/step - loss: 5.0231 - accuracy: 0.1486 - val_loss: 5.7783 - val_accuracy: 0.1580\n",
      "Epoch 11/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.9552 - accuracy: 0.1522 - val_loss: 5.7723 - val_accuracy: 0.1592\n",
      "Epoch 12/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.8921 - accuracy: 0.1558 - val_loss: 5.7801 - val_accuracy: 0.1629\n",
      "Epoch 13/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.8333 - accuracy: 0.1601 - val_loss: 5.7765 - val_accuracy: 0.1678\n",
      "Epoch 14/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.7779 - accuracy: 0.1627 - val_loss: 5.7943 - val_accuracy: 0.1674\n",
      "Epoch 15/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.7244 - accuracy: 0.1650 - val_loss: 5.7586 - val_accuracy: 0.1708\n",
      "Epoch 16/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.6759 - accuracy: 0.1681 - val_loss: 5.7910 - val_accuracy: 0.1734\n",
      "Epoch 17/50\n",
      "325/325 [==============================] - 149s 459ms/step - loss: 4.6292 - accuracy: 0.1693 - val_loss: 5.7880 - val_accuracy: 0.1779\n",
      "Epoch 18/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.5868 - accuracy: 0.1709 - val_loss: 5.8210 - val_accuracy: 0.1727\n",
      "Epoch 19/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.5434 - accuracy: 0.1731 - val_loss: 5.8273 - val_accuracy: 0.1779\n",
      "Epoch 20/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.5036 - accuracy: 0.1749 - val_loss: 5.8646 - val_accuracy: 0.1838\n",
      "Epoch 21/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.4661 - accuracy: 0.1765 - val_loss: 5.8814 - val_accuracy: 0.1863\n",
      "Epoch 22/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.4295 - accuracy: 0.1777 - val_loss: 5.8942 - val_accuracy: 0.1873\n",
      "Epoch 23/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.3964 - accuracy: 0.1787 - val_loss: 5.9447 - val_accuracy: 0.1838\n",
      "Epoch 24/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.3637 - accuracy: 0.1803 - val_loss: 5.9561 - val_accuracy: 0.1943\n",
      "Epoch 25/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.3341 - accuracy: 0.1820 - val_loss: 5.9947 - val_accuracy: 0.1880\n",
      "Epoch 26/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.3075 - accuracy: 0.1825 - val_loss: 5.9971 - val_accuracy: 0.1914\n",
      "Epoch 27/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.2820 - accuracy: 0.1836 - val_loss: 6.0373 - val_accuracy: 0.1984\n",
      "Epoch 28/50\n",
      "325/325 [==============================] - 148s 456ms/step - loss: 4.2598 - accuracy: 0.1849 - val_loss: 6.0565 - val_accuracy: 0.1967\n",
      "Epoch 29/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.2380 - accuracy: 0.1853 - val_loss: 6.1112 - val_accuracy: 0.1958\n",
      "Epoch 30/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.2182 - accuracy: 0.1860 - val_loss: 6.0937 - val_accuracy: 0.1929\n",
      "Epoch 31/50\n",
      "325/325 [==============================] - 148s 457ms/step - loss: 4.2000 - accuracy: 0.1866 - val_loss: 6.1339 - val_accuracy: 0.1960\n",
      "Epoch 32/50\n",
      "325/325 [==============================] - 148s 456ms/step - loss: 4.1842 - accuracy: 0.1866 - val_loss: 6.1835 - val_accuracy: 0.1952\n",
      "Epoch 33/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.1691 - accuracy: 0.1866 - val_loss: 6.1782 - val_accuracy: 0.1977\n",
      "Epoch 34/50\n",
      "325/325 [==============================] - 148s 456ms/step - loss: 4.1537 - accuracy: 0.1871 - val_loss: 6.1926 - val_accuracy: 0.1974\n",
      "Epoch 35/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.1414 - accuracy: 0.1875 - val_loss: 6.2359 - val_accuracy: 0.1970\n",
      "Epoch 36/50\n",
      "325/325 [==============================] - 149s 457ms/step - loss: 4.1284 - accuracy: 0.1876 - val_loss: 6.2424 - val_accuracy: 0.1926\n",
      "Epoch 37/50\n",
      "325/325 [==============================] - 150s 460ms/step - loss: 4.1157 - accuracy: 0.1865 - val_loss: 6.3093 - val_accuracy: 0.2002\n",
      "Epoch 38/50\n",
      "325/325 [==============================] - 149s 459ms/step - loss: 4.1051 - accuracy: 0.1882 - val_loss: 6.2867 - val_accuracy: 0.1946\n",
      "Epoch 39/50\n",
      "325/325 [==============================] - 149s 460ms/step - loss: 4.0945 - accuracy: 0.1879 - val_loss: 6.3374 - val_accuracy: 0.2008\n",
      "Epoch 40/50\n",
      "325/325 [==============================] - 149s 459ms/step - loss: 4.0855 - accuracy: 0.1874 - val_loss: 6.3106 - val_accuracy: 0.2004\n",
      "Epoch 41/50\n",
      "325/325 [==============================] - 150s 460ms/step - loss: 4.0757 - accuracy: 0.1880 - val_loss: 6.3701 - val_accuracy: 0.1935\n",
      "Epoch 42/50\n",
      "325/325 [==============================] - 149s 460ms/step - loss: 4.0672 - accuracy: 0.1884 - val_loss: 6.3858 - val_accuracy: 0.1945\n",
      "Epoch 43/50\n",
      "325/325 [==============================] - 149s 460ms/step - loss: 4.0584 - accuracy: 0.1880 - val_loss: 6.4035 - val_accuracy: 0.2034\n",
      "Epoch 44/50\n",
      "325/325 [==============================] - 150s 461ms/step - loss: 4.0501 - accuracy: 0.1882 - val_loss: 6.4478 - val_accuracy: 0.1983\n",
      "Epoch 45/50\n",
      "325/325 [==============================] - 150s 461ms/step - loss: 4.0417 - accuracy: 0.1883 - val_loss: 6.4716 - val_accuracy: 0.2059\n",
      "Epoch 46/50\n",
      "325/325 [==============================] - 150s 460ms/step - loss: 4.0349 - accuracy: 0.1880 - val_loss: 6.4819 - val_accuracy: 0.2001\n",
      "Epoch 47/50\n",
      "325/325 [==============================] - 149s 460ms/step - loss: 4.0279 - accuracy: 0.1875 - val_loss: 6.5184 - val_accuracy: 0.2005\n",
      "Epoch 48/50\n",
      "325/325 [==============================] - 150s 460ms/step - loss: 4.0213 - accuracy: 0.1880 - val_loss: 6.5197 - val_accuracy: 0.1960\n",
      "Epoch 49/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.0155 - accuracy: 0.1881 - val_loss: 6.5545 - val_accuracy: 0.2056\n",
      "Epoch 50/50\n",
      "325/325 [==============================] - 149s 458ms/step - loss: 4.0098 - accuracy: 0.1889 - val_loss: 6.4935 - val_accuracy: 0.2012\n"
     ]
    }
   ],
   "source": [
    "#Defining the model, a recurrent neural network\n",
    "next_words_model = keras.models.Sequential([\n",
    "                                            keras.layers.Embedding(vocab_size, 10, input_length=1),\n",
    "                                            keras.layers.LSTM(1000, return_sequences=True),\n",
    "                                            keras.layers.LSTM(1000),\n",
    "                                            keras.layers.Dense(500, activation = 'relu'),\n",
    "                                            keras.layers.Dense(vocab_size, activation = 'softmax')\n",
    "])\n",
    "next_words_model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = next_words_model.fit(X, y, epochs=50, batch_size=128, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dMVEtPbEArt",
    "outputId": "61ad0287-900f-4bdf-91c1-baee5ffc9e58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_tokenizer.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_words_model.save('next_words_model.h5')\n",
    "joblib.dump(tokenizer, 'text_tokenizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "VxN8krBiB7of"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "\n",
    "model = keras.models.load_model('next_words_model.h5')\n",
    "tokenizer = joblib.load('text_tokenizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "bUKAB4lZCbGe"
   },
   "outputs": [],
   "source": [
    "def retrain_model(text):\n",
    "  retrain_seq = tokenizer.texts_to_sequences([text])[0]\n",
    "  retrain_data = []\n",
    "  for i in range(1, len(retrain_seq)):\n",
    "    retrain_data.append(retrain_seq[i-1:i+1])\n",
    "  X_train = np.array([v[0] for v in retrain_data])\n",
    "  y_train = np.array([v[1] for v in retrain_data])\n",
    "  y_train = keras.utils.to_categorical(y_train, num_classes=vocab_size)\n",
    "  model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "nyDtQhecHAEd"
   },
   "outputs": [],
   "source": [
    "def predict_next_word(text):\n",
    "  try:\n",
    "    text = text.split(\" \")\n",
    "    text = text[-1]\n",
    "  \n",
    "  except:\n",
    "    print('Invalid Entry')\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "  sequence = np.array(sequence)\n",
    "\n",
    "  preds = model.predict(sequence)\n",
    "  pred_words = tokenizer.sequences_to_texts([sorted(np.argsort(preds)[0][-3:])]) #getting the words with the highest probabilities\n",
    "\n",
    "  pred_words = pred_words[0].split(' ')\n",
    "  print(\"The most probable word is: {} \\n Other likely words: {}, {}\".format(pred_words[0], pred_words[1], pred_words[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ATBLLZ_IJdC"
   },
   "outputs": [],
   "source": [
    "retrain_model('My hand hurts like hell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJWm3CDKVL1U",
    "outputId": "6dcf7e2a-e8d0-495e-cbf6-0c1dda01b919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable word is: is \n",
      " Other likely words: lucy, hurts\n"
     ]
    }
   ],
   "source": [
    "predict_next_word('hand')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Next_word_predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
